(* Lexer para YAPar *)

{
grammar = dict()
yapar_tokens = set()
}

(* Comentarios *)
let comment = /\*['A'-'Z''a'-'z'"ÁÉÍÓÚ""áéíóú"'0'-'9'"\s"",_+-.?!$~`|/:;=<>#^@\""\[\]"]*\*/

(* Espacios en blanco *)
let delim = [' ''\t''\n']
let ws = delim+

(* Simplificaciones *)
let letter = ['A'-'Z''a'-'z']
let digit = ['0'-'9']
let id = (letter)(letter|digit)*
let minusId = ['a'-'z'](['a'-'z']|digit)*
let value = ([' ''\t']+id)+
let term = ([' ''\t''\n']*id)+

(* Seccion de Tokens *)
let tokenDefinition = "%token"(value)
let ignoreDefinition = "IGNORE"(value)

(* Seccion de Producciones *)
let productionSection = "%%"
let production = (minusId)':'(term)?(ws)*('|'(term)?(ws)*)*';'

rule tokens = 
  | comment		{ return "COMMENT" }
  | ws			{ return "WHITESPACE" }
  | tokenDefinition
{
for item in value[6:].split(' '):
	if item!='':
		yapar_tokens.add(item)
}
  | ignoreDefinition	{ return "IGNOREDEF" }
  | productionSection	{ return "PRODSECTION" }
  | production
{prod = value[:-1].split(':')
head = prod[0]
body = []
for item in prod[1].split('|'):
	body.append(item.strip())
grammar[head] = body
}

{
with open('grammar.pkl', 'wb') as archivo_salida:
        # Serializamos el diccionario y lo guardamos en el archivo
        pickle.dump(grammar, archivo_salida)
}
